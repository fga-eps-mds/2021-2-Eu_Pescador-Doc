{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 2021/02\n",
    "\n",
    "#### SUMMARY:\n",
    "\n",
    "- This notebook represents the project quality analysis of the date exposed right above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEAM:\n",
    "\n",
    "##### Semester: 2021/02\n",
    "##### Professor: Hilmer Neri\n",
    "\n",
    "##### Members:\n",
    "\n",
    "- 18/0013637 Arthur Paiva Tavares\n",
    "- 17/0031538 Dâmaso Júnio pereira Brasileo\n",
    "- 18/0063723 Fellipe dos Santos Araujo\n",
    "- 15/0135939 Letícia Karla Araújo\n",
    "- 20/0057227 Caio Vitor de Oliveira\n",
    "- 20/0036351 Clara Marcelino de Sousa\n",
    "- 20/0030469 Felipe Candido de Moura\n",
    "- 19/0091720 Lucas macedo Barboza\n",
    "- 20/0023411 Luíza Esteves dos Santos\n",
    "- 20/0041959 Maurício Machado Filho\n",
    "- 19/0037806 Samuel Furtado Avila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# Deal with API request\n",
    "import urllib3\n",
    "from urllib3 import request\n",
    "\n",
    "# Deal with visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deal with time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('darkgrid',\n",
    "              {'xtick.bottom' : True,\n",
    "               'ytick.left': True,\n",
    "               'grid.linestyle':'--',\n",
    "               'font.monospace': ['Computer Modern Typewriter'],\n",
    "               'axes.edgecolor' : 'white'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SonarCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path to the folder with all your jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = glob('../analytics-raw-data/*.json') # add the path here\n",
    "print(jsons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_path):\n",
    "    \n",
    "    with open(json_path) as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        \n",
    "    return json_obj\n",
    "\n",
    "def create_base_component_df(json_list):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in json_list:\n",
    "\n",
    "        base_component = read_json(i)\n",
    "\n",
    "        base_component_data = base_component['baseComponent']['measures']\n",
    "\n",
    "        base_component_df = pd.DataFrame(base_component_data)\n",
    "\n",
    "        base_component_df['filename'] = os.path.basename(i)\n",
    "\n",
    "        df = df.append(base_component_df, ignore_index=True)\n",
    "        \n",
    "    aux_df = df['filename'].str.split(r\"Eu-Pescador-(.*?)-(.*?).json\", expand=True)\n",
    "    \n",
    "    df['repository'] = aux_df[1]\n",
    "    \n",
    "    df['version'] = aux_df[2]\n",
    "    \n",
    "    df = df.sort_values(by=['repository', 'version'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create base component dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_component_df = create_base_component_df(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_component_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = ['files',\n",
    "               'functions',\n",
    "               'complexity',\n",
    "               'comment_lines_density',\n",
    "               'duplicated_lines_density',\n",
    "               'coverage',\n",
    "               'ncloc',\n",
    "               'tests',\n",
    "               'test_errors',\n",
    "               'test_failures',\n",
    "               'test_execution_time',\n",
    "               'security_rating']\n",
    "\n",
    "len(metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_per_file(json):\n",
    "    \n",
    "    file_json = []\n",
    "    \n",
    "    for component in json['components']:\n",
    "        if component['qualifier'] == 'FIL' or component['qualifier'] == 'UTS':\n",
    "            file_json.append(component)\n",
    "            \n",
    "    return file_json\n",
    "\n",
    "def generate_file_dataframe_per_release(metric_list, json, language_extension):\n",
    "    \n",
    "    df_columns = metric_list\n",
    "    df = pd.DataFrame(columns = df_columns)\n",
    "    \n",
    "    for file in json:\n",
    "        try:\n",
    "            if file['language'] == language_extension:\n",
    "                for measure in file['measures']:\n",
    "                    df.at[file['path'], measure['metric']] = measure['value']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    df.reset_index(inplace = True)\n",
    "    df = df.rename({'index': 'path'}, axis=1).drop(['files'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_file_df(json_list):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in json_list:\n",
    "\n",
    "        file_component = read_json(i)\n",
    "        \n",
    "        file_component_data = metric_per_file(file_component)\n",
    "\n",
    "        file_component_df = generate_file_dataframe_per_release(metric_list, file_component_data, language_extension = 'ts')\n",
    "\n",
    "        file_component_df['filename'] = os.path.basename(i)\n",
    "        df = df.append(file_component_df, ignore_index=True)\n",
    "        \n",
    "    # replace TeamName by yours.    \n",
    "    aux_df = df['filename'].str.split(r\"Eu-Pescador-(.*?)-(.*?).json\", expand=True)\n",
    "    \n",
    "    df['repository'] = aux_df[1]\n",
    "    \n",
    "    df['version'] = aux_df[2]\n",
    "    \n",
    "    df = df.sort_values(by=['repository', 'version'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_component_df = create_file_df(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_component_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create dataframe per repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo1_df = file_component_df[file_component_df['repository'] == 'UserInterface']\n",
    "repo2_df = file_component_df[file_component_df['repository'] == 'User']\n",
    "repo3_df = file_component_df[file_component_df['repository'] == 'FishWiki']\n",
    "repo4_df = file_component_df[file_component_df['repository'] == 'FishLog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _ncloc(df):\n",
    "    ncloc = 0\n",
    "    for each in df['ncloc']:\n",
    "        n = 0\n",
    "        # try to cast the current ncloc value to int, if the value is NaN/Null, consider it as zero.\n",
    "        try:\n",
    "            n = int(each)\n",
    "        except ValueError:\n",
    "            n = 0\n",
    "        ncloc += n\n",
    "\n",
    "    return ncloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Function to filter files from glob string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os.path\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Cross-Python dictionary views on the keys \n",
    "if hasattr(dict, 'viewkeys'):\n",
    "    # Python 2\n",
    "    def _viewkeys(d):\n",
    "        return d.viewkeys()\n",
    "else:\n",
    "    # Python 3\n",
    "    def _viewkeys(d):\n",
    "        return d.keys()\n",
    "\n",
    "\n",
    "def _in_trie(trie, path):\n",
    "    \"\"\"Determine if path is completely in trie\"\"\"\n",
    "    current = trie\n",
    "    for elem in path:\n",
    "        try:\n",
    "            current = current[elem]\n",
    "        except KeyError:\n",
    "            return False\n",
    "    return None in current\n",
    "\n",
    "\n",
    "def find_matching_paths(paths, pattern):\n",
    "    \"\"\"Produce a list of paths that match the pattern.\n",
    "\n",
    "    * paths is a list of strings representing filesystem paths\n",
    "    * pattern is a glob pattern as supported by the fnmatch module\n",
    "\n",
    "    \"\"\"\n",
    "    if os.altsep:  # normalise\n",
    "        pattern = pattern.replace(os.altsep, os.sep)\n",
    "    pattern = pattern.split(os.sep)\n",
    "\n",
    "    # build a trie out of path elements; efficiently search on prefixes\n",
    "    path_trie = {}\n",
    "    for path in paths:\n",
    "        if os.altsep:  # normalise\n",
    "            path = path.replace(os.altsep, os.sep)\n",
    "        _, path = os.path.splitdrive(path)\n",
    "        elems = path.split(os.sep)\n",
    "        current = path_trie\n",
    "        for elem in elems:\n",
    "            current = current.setdefault(elem, {})\n",
    "        current.setdefault(None, None)  # sentinel\n",
    "\n",
    "    matching = []\n",
    "\n",
    "    current_level = [path_trie]\n",
    "    for subpattern in pattern:\n",
    "        if not glob.has_magic(subpattern):\n",
    "            # plain element, element must be in the trie or there are\n",
    "            # 0 matches\n",
    "            if not any(subpattern in d for d in current_level):\n",
    "                return []\n",
    "            matching.append([subpattern])\n",
    "            current_level = [d[subpattern] for d in current_level if subpattern in d]\n",
    "        else:\n",
    "            # match all next levels in the trie that match the pattern\n",
    "            matched_names = fnmatch.filter({k for d in current_level for k in d}, subpattern)\n",
    "            if not matched_names:\n",
    "                # nothing found\n",
    "                return []\n",
    "            matching.append(matched_names)\n",
    "            current_level = [d[n] for d in current_level for n in _viewkeys(d) & set(matched_names)]\n",
    "\n",
    "    return [os.sep.join(p) for p in product(*matching)\n",
    "            if _in_trie(path_trie, p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure calculations according Q-Rapids quality model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Aspect - Maintainability\n",
    "## Factor - Code Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1(df):\n",
    "    invalid_complexity_value = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['complexity']) or pd.isna(row['functions']):\n",
    "            invalid_complexity_value += 1\n",
    "    \n",
    "    density_non_complex_files = len(df[(df['complexity'].astype(float)/df['functions'].astype(float)) < 10])/max(len(df) - invalid_complexity_value, 1)\n",
    "    \n",
    "    return density_non_complex_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2(df):\n",
    "    \n",
    "    density_of_commented_files = len(df[(df['comment_lines_density'].astype(float) > 10) & (df['comment_lines_density'].astype(float) < 30)])/len(df)\n",
    "    \n",
    "    return density_of_commented_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DUPLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m3(df):\n",
    "    \n",
    "    absence_of_duplications = len(df[(df['duplicated_lines_density'].astype(float) < 5)])/len(df)\n",
    "    \n",
    "    return absence_of_duplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Aspect - Reliability\n",
    "## Factor - Testing Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Errors tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m4(df):\n",
    "\n",
    "    passed_tests = df[pd.notna(df['test_errors'])]['test_errors'].astype(float).median() / 100\n",
    "\n",
    "    return passed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Failure tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m5(df):\n",
    "\n",
    "    passed_tests = df[pd.notna(df['test_failures'])]['test_failures'].astype(float).median() / 100\n",
    "\n",
    "    return passed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fast test builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m6(df):\n",
    "    fast_tests_files = df[(pd.notna(df['test_execution_time'])) & ((df['test_execution_time'].astype(float)) < 300)]\n",
    "    qnt_fast_tests_per_file = sum(fast_tests_files['tests'].astype(float))\n",
    "    density_fast_test_builds = len(fast_tests_files) * qnt_fast_tests_per_file / sum(df[(pd.notna(df['tests']))]['tests'].astype(float))\n",
    "\n",
    "    return density_fast_test_builds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def m7(df):\n",
    "    invalid_paths = [\"test/**/*.spec.ts\", \"**/test/**/*.spec.ts\", \"**test/**/*.spec.ts\", \"src/app.ts\", \"src/server.ts\", \"src/routes/router.ts\", \"src/config/database.ts\"]\n",
    "    invalid_test_value = 0\n",
    "    paths = []\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['coverage']):\n",
    "            invalid_test_value += 1\n",
    "        else:\n",
    "            paths.append(row['path'])\n",
    "\n",
    "    for pattern in invalid_paths:\n",
    "        matching = find_matching_paths(paths, pattern)\n",
    "        invalid_test_value += len(matching)\n",
    "\n",
    "    density_test_coverage = len(df[(df['coverage'].astype(float) > 60)]) / max((len(df) - invalid_test_value), 1)\n",
    "\n",
    "    return density_test_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate m1, m2, m3, m4, m5 and m6 for each repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(df):\n",
    "    version_vec = df['version'].unique()\n",
    "    \n",
    "    m1_list = []\n",
    "    m2_list = []\n",
    "    m3_list = []\n",
    "    m4_list = []\n",
    "    m5_list = []\n",
    "    m6_list = []\n",
    "    m7_list = []\n",
    "\n",
    "    ncloc_list = []\n",
    "    repository_list = []\n",
    "    version_list = []\n",
    "    \n",
    "    metrics_df = pd.DataFrame()\n",
    "    for version in version_vec:\n",
    "        version_df = df[df['version'] == version]\n",
    "\n",
    "        m1_list.append(m1(version_df))\n",
    "        m2_list.append(m2(version_df))\n",
    "        m3_list.append(m3(version_df))\n",
    "        m4_list.append(m4(version_df))\n",
    "        m5_list.append(m5(version_df))\n",
    "        m6_list.append(m6(version_df))\n",
    "        m7_list.append(m7(version_df))\n",
    "\n",
    "        ncloc_list.append(_ncloc(version_df))\n",
    "        repository_list.append(version_df['repository'].iloc[0])\n",
    "        version_list.append(version)\n",
    "        \n",
    "    metrics_df = pd.DataFrame({'m1': m1_list,\n",
    "                               'm2': m2_list,\n",
    "                               'm3': m3_list,\n",
    "                               'm4': m4_list,\n",
    "                               'm5': m5_list,\n",
    "                               'm6': m6_list,\n",
    "                               'm7': m7_list,\n",
    "                               'repository': repository_list, \n",
    "                               'version': version_list,\n",
    "                               'ncloc': ncloc_list})\n",
    "        \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo1_metrics = create_metrics_df(repo1_df)\n",
    "repo2_metrics = create_metrics_df(repo2_df)\n",
    "repo3_metrics = create_metrics_df(repo3_df)\n",
    "repo4_metrics = create_metrics_df(repo4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data visualization\n",
    "\n",
    "- You must do this for each of your repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('UserInterface Metrics')\n",
    "plt.plot(repo1_metrics['m1'], linewidth=3, marker='o', label='Complexity', markersize=10)\n",
    "plt.plot(repo1_metrics['m2'], linewidth=3, marker='s', label='Comment Lines Density', markersize=10)\n",
    "plt.plot(repo1_metrics['m3'], linewidth=3, marker='^', label='Absence of Duplications', markersize=10)\n",
    "plt.plot(repo1_metrics['m4'], linewidth=3, marker='x', label='Test Errors', markersize=10)\n",
    "plt.plot(repo1_metrics['m5'], linewidth=3, marker='<', label='Test Failures', markersize=10)\n",
    "plt.plot(repo1_metrics['m6'], linewidth=3, marker='>', label='Test Execution Time', markersize=10)\n",
    "plt.plot(repo1_metrics['m7'], linewidth=3, marker='v', label='Coverage', markersize=10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('User Metrics')\n",
    "plt.plot(repo2_metrics['m1'], linewidth=3, marker='o', label='Complexity', markersize=10)\n",
    "plt.plot(repo2_metrics['m2'], linewidth=3, marker='s', label='Comment Lines Density', markersize=10)\n",
    "plt.plot(repo2_metrics['m3'], linewidth=3, marker='^', label='Absence of Duplications', markersize=10)\n",
    "plt.plot(repo2_metrics['m4'], linewidth=3, marker='x', label='Test Errors', markersize=10)\n",
    "plt.plot(repo2_metrics['m5'], linewidth=3, marker='<', label='Test Failures', markersize=10)\n",
    "plt.plot(repo2_metrics['m6'], linewidth=3, marker='>', label='Test Execution Time', markersize=10)\n",
    "plt.plot(repo2_metrics['m7'], linewidth=3, marker='v', label='Coverage', markersize=10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('FishWiki Metrics')\n",
    "plt.plot(repo3_metrics['m1'], linewidth=3, marker='o', label='Complexity', markersize=10)\n",
    "plt.plot(repo3_metrics['m2'], linewidth=3, marker='s', label='Comment Lines Density', markersize=10)\n",
    "plt.plot(repo3_metrics['m3'], linewidth=3, marker='^', label='Absence of Duplications', markersize=10)\n",
    "plt.plot(repo3_metrics['m4'], linewidth=3, marker='x', label='Test Errors', markersize=10)\n",
    "plt.plot(repo3_metrics['m5'], linewidth=3, marker='<', label='Test Failures', markersize=10)\n",
    "plt.plot(repo3_metrics['m6'], linewidth=3, marker='>', label='Test Execution Time', markersize=10)\n",
    "plt.plot(repo3_metrics['m7'], linewidth=3, marker='v', label='Coverage', markersize=10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('FishLog Metrics')\n",
    "plt.plot(repo4_metrics['m1'], linewidth=3, marker='o', label='Complexity', markersize=10)\n",
    "plt.plot(repo4_metrics['m2'], linewidth=3, marker='s', label='Comment Lines Density', markersize=10)\n",
    "plt.plot(repo4_metrics['m3'], linewidth=3, marker='^', label='Absence of Duplications', markersize=10)\n",
    "plt.plot(repo4_metrics['m4'], linewidth=3, marker='x', label='Test Errors', markersize=10)\n",
    "plt.plot(repo4_metrics['m5'], linewidth=3, marker='<', label='Test Failures', markersize=10)\n",
    "plt.plot(repo4_metrics['m6'], linewidth=3, marker='>', label='Test Execution Time', markersize=10)\n",
    "plt.plot(repo4_metrics['m7'], linewidth=3, marker='v', label='Coverage', markersize=10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub characteristic aggregation\n",
    "\n",
    "- You must do this for each of your repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "psc1 = 1\n",
    "psc2 = 1\n",
    "pc1 = 0.5\n",
    "pc2 = 0.5\n",
    "pm1 = 0.33\n",
    "pm2 = 0.33\n",
    "pm3 = 0.33\n",
    "pm4 = 0.15\n",
    "pm5 = 0.15\n",
    "pm6 = 0.15\n",
    "pm7 = 0.7\n",
    "\n",
    "repo1_metrics['code_quality'] = ((repo1_metrics['m1']*pm1) + (repo1_metrics['m2']*pm2) + (repo1_metrics['m3']*pm3)) * psc1\n",
    "repo2_metrics['code_quality'] = ((repo2_metrics['m1']*pm1) + (repo2_metrics['m2']*pm2) + (repo2_metrics['m3']*pm3)) * psc1\n",
    "repo3_metrics['code_quality'] = ((repo3_metrics['m1']*pm1) + (repo3_metrics['m2']*pm2) + (repo3_metrics['m3']*pm3)) * psc1\n",
    "repo4_metrics['code_quality'] = ((repo4_metrics['m1']*pm1) + (repo4_metrics['m2']*pm2) + (repo4_metrics['m3']*pm3)) * psc1\n",
    "\n",
    "repo1_metrics['testing_status'] = ((repo1_metrics['m4']*pm4) + (repo1_metrics['m5']*pm5) + (repo1_metrics['m6']*pm6) + (repo1_metrics['m7']*pm7)) * psc2\n",
    "repo2_metrics['testing_status'] = ((repo2_metrics['m4']*pm4) + (repo2_metrics['m5']*pm5) + (repo2_metrics['m6']*pm6) + (repo2_metrics['m7']*pm7)) * psc2\n",
    "repo3_metrics['testing_status'] = ((repo3_metrics['m4']*pm4) + (repo3_metrics['m5']*pm5) + (repo3_metrics['m6']*pm6) + (repo3_metrics['m7']*pm7)) * psc2\n",
    "repo4_metrics['testing_status'] = ((repo4_metrics['m4']*pm4) + (repo4_metrics['m5']*pm5) + (repo4_metrics['m6']*pm6) + (repo4_metrics['m7']*pm7)) * psc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('Code Quality')\n",
    "plt.plot(repo1_metrics['code_quality'], linewidth=3, marker='o', label='UserInterface', markersize=5)\n",
    "plt.plot(repo2_metrics['code_quality'], linewidth=3, marker='s', label='User', markersize=5)\n",
    "plt.plot(repo3_metrics['code_quality'], linewidth=3, marker='^', label='FishWiki', markersize=5)\n",
    "plt.plot(repo4_metrics['code_quality'], linewidth=3, marker='<', label='FishLog', markersize=5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('Testing Status')\n",
    "plt.plot(repo1_metrics['testing_status'], linewidth=3, marker='o', label='UserInterface', markersize=5)\n",
    "plt.plot(repo2_metrics['testing_status'], linewidth=3, marker='s', label='User', markersize=5)\n",
    "plt.plot(repo3_metrics['testing_status'], linewidth=3, marker='^', label='FishWiki', markersize=5)\n",
    "plt.plot(repo4_metrics['testing_status'], linewidth=3, marker='<', label='FishLog', markersize=5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.concat([repo1_metrics, repo2_metrics, repo3_metrics, repo4_metrics], ignore_index=True)\n",
    "\n",
    "metrics_df['maintainability'] = metrics_df['code_quality'] * pc1\n",
    "metrics_df['reliability'] = metrics_df['testing_status'] * pc2\n",
    "metrics_df['total'] = metrics_df['maintainability'] + metrics_df['reliability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "plt.plot(metrics_df['maintainability'], linewidth=3, marker='o', label='Maintainability', markersize=5)\n",
    "plt.plot(metrics_df['reliability'], linewidth=3, marker='s', label='Reliability', markersize=5)\n",
    "plt.legend()\n",
    "# plt.ylim(.2,.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('Total')\n",
    "plt.plot(metrics_df['total'], linewidth=3, marker='o', markersize=5)\n",
    "# plt.ylim(.65,.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS_DATE FORMAT: YYYY-MM-DD\n",
    "currentTime = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# metrics_df.to_excel('data/fga-eps-mds-Eu-Pescador-DATASET-{}.xlsx'.format(currentTime), index = False)\n",
    "metrics_df.to_csv('data/fga-eps-mds-Eu-Pescador-DATASET-{}.csv'.format(currentTime), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics_df = pd.DataFrame()\n",
    "\n",
    "descriptive_statistics_df['mode_reliability'] = metrics_df['reliability'].mode()\n",
    "descriptive_statistics_df['mode_maintainability'] = metrics_df['maintainability'].mode()\n",
    "descriptive_statistics_df['median_reliability'] = metrics_df['reliability'].median()\n",
    "descriptive_statistics_df['median_maintainability'] = metrics_df['maintainability'].median()\n",
    "descriptive_statistics_df['mean_reliability'] = metrics_df['reliability'].mean()\n",
    "descriptive_statistics_df['mean_maintainability'] = metrics_df['maintainability'].mean()\n",
    "descriptive_statistics_df['max_reliability'] = metrics_df['reliability'].max()\n",
    "descriptive_statistics_df['max_maintainability'] = metrics_df['maintainability'].max()\n",
    "descriptive_statistics_df['min_reliability'] = metrics_df['reliability'].min()\n",
    "descriptive_statistics_df['min_maintainability'] = metrics_df['maintainability'].min()\n",
    "descriptive_statistics_df['std_dev_reliability'] = metrics_df['reliability'].std()\n",
    "descriptive_statistics_df['std_dev_maintainability'] = metrics_df['maintainability'].std()\n",
    "descriptive_statistics_df['variance_reliability'] = metrics_df['reliability'].var()\n",
    "descriptive_statistics_df['variance_maintainability'] = metrics_df['maintainability'].var()\n",
    "\n",
    "descriptive_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics_archives = pd.DataFrame()\n",
    "\n",
    "descriptive_statistics_archives['mode_reliability'] = metrics_df['testing_status'].mode()\n",
    "descriptive_statistics_archives['mode_maintainability'] = metrics_df['code_quality'].mode()\n",
    "descriptive_statistics_archives['median_reliability'] = metrics_df['testing_status'].median()\n",
    "descriptive_statistics_archives['median_maintainability'] = metrics_df['code_quality'].median()\n",
    "descriptive_statistics_archives['mean_reliability'] = metrics_df['testing_status'].mean()\n",
    "descriptive_statistics_archives['mean_maintainability'] = metrics_df['code_quality'].mean()\n",
    "descriptive_statistics_archives['max_reliability'] = metrics_df['testing_status'].max()\n",
    "descriptive_statistics_archives['max_maintainability'] = metrics_df['code_quality'].max()\n",
    "descriptive_statistics_archives['min_reliability'] = metrics_df['testing_status'].min()\n",
    "descriptive_statistics_archives['min_maintainability'] = metrics_df['code_quality'].min()\n",
    "descriptive_statistics_archives['std_dev_reliability'] = metrics_df['testing_status'].std()\n",
    "descriptive_statistics_archives['std_dev_maintainability'] = metrics_df['code_quality'].std()\n",
    "descriptive_statistics_archives['variance_reliability'] = metrics_df['testing_status'].var()\n",
    "descriptive_statistics_archives['variance_maintainability'] = metrics_df['code_quality'].var()\n",
    "\n",
    "descriptive_statistics_archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "heatmap = sns.heatmap(metrics_df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Archives Correlation Heatmap', fontdict={'fontsize':18}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title(\"Code Quality bars\")\n",
    "plt.ylabel(\"Number of repetitions of % value\")\n",
    "plt.xlabel(\"Code Quality percentage\")\n",
    "plt.hist(metrics_df['code_quality'], bins=10, color='c', alpha=0.65)\n",
    "plt.axvline(metrics_df['code_quality'].mean(), color='k', linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title(\"Testing Status bars\")\n",
    "plt.ylabel(\"Number of repetitions of % value\")\n",
    "plt.xlabel(\"Testing Status percentage\")\n",
    "plt.hist(metrics_df['testing_status'], bins=10, color='c', alpha=0.65)\n",
    "plt.axvline(metrics_df['testing_status'].mean(), color='k', linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title(\"Code Quality bloxplot\")\n",
    "plt.boxplot(metrics_df['code_quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title(\"Testing Status boxplot\")\n",
    "plt.boxplot(metrics_df['testing_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
